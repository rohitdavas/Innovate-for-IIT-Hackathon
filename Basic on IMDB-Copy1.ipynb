{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:34:21.873092Z",
     "start_time": "2020-04-14T06:34:21.725925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) \n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "    print(imdb_data_path)\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the word embeedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rohit/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    # Vectorization parameters\n",
    "    # Limit on the number of features. We use the top 20K features.\n",
    "    TOP_K = 20000\n",
    "    # Limit on the length of text sequences. Sequences longer than this\n",
    "    # will be truncated.\n",
    "    MAX_SEQUENCE_LENGTH = 500\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index,tokenizer\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_imdb_sentiment_analysis_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f424f7488a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/rohit/Documents/Study/Projects/HACKATHON INNOVATE FOR IIT/aclImdb_v1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_imdb_sentiment_analysis_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_imdb_sentiment_analysis_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "# get the sequences\n",
    "#define useful global variables\n",
    "TOP_K = 20000\n",
    "MAX_SEQUENCE_LENGTH  = 500\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "data_path = '/home/rohit/Documents/Study/Projects/HACKATHON INNOVATE FOR IIT/aclImdb_v1'\n",
    "(train_texts, train_labels), (test_texts, test_labels) = load_imdb_sentiment_analysis_dataset(data_path)\n",
    "\n",
    "#sequences\n",
    "x_train, x_test, tokenizer_word_index,tokenizer = sequence_vectorize(train_texts, test_texts)\n",
    "\n",
    "\n",
    "num_words = min(TOP_K, len(tokenizer_word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"POSSIBLE SPOILERS<br /><br />The Spy Who Shagged Me is a muchly overrated and over-hyped sequel. International Man of Mystery came straight out of the blue. It was a lone star that few people had heard of. But it was stunningly original, had sophisticated humour and ample humour, always kept in good taste, and had a brilliant cast. The Spy Who Shagged Me was a lot more commercially advertised and hyped about.<br /><br />OK I'll admit, the first time I saw this film I thought it was very funny, but it's only after watching it two or three times that you see all the flaws. The acting was OK, but Heather Graham cannot act. Her performance didn't seem very convincing and she wasn't near as good as Liz Hurley was in the first one. Those characters who bloomed in the first one, (Scott Evil, Number 2 etc.) are thrown into the background hear and don't get many stand-alone scenes. The film is simply overrun with cameos.<br /><br />In particular, I hated the way they totally disregarded some of the scenes in IMOM. When they killed off Vanessa at the start and had Basil sat that he knew she was a fembot all along. What was the point of that? They killed off Number 2 in the first one, and now they bring him back with no explanation whatsoever. This is supposed to be a spy-spoof, I don't think any of the characters even hold a gun in the film. It just goes on a trail, further and further away from the point.<br /><br />The new characters are very unwelcome. The whole Mini-Me `make fun of my size' joke gets old very quickly. Fat Bastard is just a lame excuse for gross-out humour. In total there's about two or three good jokes. The rest are either tasteless or rehashed from IMOM.<br /><br />If this were the first movie of the series then I'd probably be easier on it. But the series started on a note of dry wit and then plummeted down to a level of gross out humour. So I say, only watch this film if you haven't seen its predecessor, because The Spy Who Shagged Me is one ultimate disappointment.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#get the glove embeedings\n",
    "glove_dir = '/home/rohit/Documents/Study/Projects/HACKATHON INNOVATE FOR IIT/'\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_word_index.items():\n",
    "    if i >= TOP_K:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#keras layers import\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras import models, initializers, regularizers\n",
    "from keras.layers import Dense, Dropout, SeparableConv1D, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        \n",
    "        filters: int, output dimension of the layers.\n",
    "        \n",
    "        kernel_size: int, length of the convolution window.\n",
    "        \n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        \n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        \n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        \n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        \n",
    "        num_classes: int, number of output classes.\n",
    "        \n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        \n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        \n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        \n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepcnn_model(blocks,\n",
    "                 filters = 64,\n",
    "                 kernel_size = 3,\n",
    "                 embedding_dim = EMBEDDING_DIM, \n",
    "                 dropout_rate = 0.2,\n",
    "                 pool_size =2, \n",
    "                 MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH,\n",
    "                 num_classes = 2,\n",
    "                 num_features = num_words,\n",
    "                 use_pretrained_embedding=True,\n",
    "                 is_embedding_trainable=True,\n",
    "                 embedding_matrix= embedding_matrix):\n",
    "    \n",
    "    op_units, op_activation = 1, 'sigmoid'\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        \n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_25 (Separab (None, 500, 64)           6764      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_26 (Separab (None, 500, 64)           4352      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_27 (Separab (None, 250, 64)           4352      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_28 (Separab (None, 250, 64)           4352      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_29 (Separab (None, 125, 64)           4352      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_30 (Separab (None, 125, 64)           4352      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_31 (Separab (None, 62, 64)            4352      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_32 (Separab (None, 62, 64)            4352      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 31, 64)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_33 (Separab (None, 31, 128)           8512      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_34 (Separab (None, 31, 128)           16896     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,062,765\n",
      "Trainable params: 2,062,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "temp = sepcnn_model(blocks= 5)\n",
    "temp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_35 (Separab (None, 500, 64)           6764      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_36 (Separab (None, 500, 64)           4352      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_37 (Separab (None, 250, 128)          8512      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_38 (Separab (None, 250, 128)          16896     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,036,653\n",
      "Trainable params: 2,036,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-3,\n",
    "epochs=1000,\n",
    "batch_size= 128,\n",
    "blocks=2,\n",
    "filters=64,\n",
    "dropout_rate= 0.2,\n",
    "embedding_dim=EMBEDDING_DIM,\n",
    "kernel_size=3,\n",
    "pool_size=3\n",
    "\n",
    "num_classes = 2\n",
    "num_features = num_words\n",
    "\n",
    "# Create model instance.\n",
    "model = sepcnn_model(blocks = 2)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 9s 373us/step - loss: 0.0038 - acc: 0.9986\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 9s 370us/step - loss: 0.0045 - acc: 0.9987\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 9s 369us/step - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 9s 372us/step - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 9s 374us/step - loss: 0.0031 - acc: 0.9990\n"
     ]
    }
   ],
   "source": [
    "# Train and validate model.\n",
    "history = model.fit(x_train, train_labels, epochs=5, verbose=1,batch_size= 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0037903949594311416, 0.004528580584379379, 0.004210526033178903, 0.007767128161918372, 0.003050965077120345] \n",
      " [0.99856, 0.99872, 0.99884, 0.99772, 0.99904]\n"
     ]
    }
   ],
   "source": [
    "#history = history.history\n",
    "history.keys()\n",
    "print(history['loss'], '\\n',history['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save model.\n",
    "model.save('IMDBmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 111us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.10974202296271, 0.86904]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "positive\n",
      "(1, 500)\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    t1 = x_test[i]\n",
    "    t1 = t1.reshape(1,t1.shape[0])\n",
    "    print(t1.shape)\n",
    "    if model.predict(t1)>0.6:\n",
    "        print('positive')\n",
    "    elif model.predict(t1)<= 0.4:\n",
    "        print('negative')\n",
    "    else:\n",
    "        print('model is not sure. in neutral state')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-898a5c9f1f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Food was very tasty. Though, they maintain less hygiene, but you can go for once and all for having tasty food while returning from tiring journey.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mess secretary please improve'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_of_single_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def sequence_of_single_sentence(sentence, tokenizer, MAX_SEQUENCE_LENGTH = 500):\n",
    "    sen = tokenizer.texts_to_sequences(sentence)\n",
    "    return sequence.pad_sequences(sen, maxlen= MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "y = ['iI admit, the great majority of films released before say 1933 are just not for me. Of the dozen or so \"major\" silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later. I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence. Early on, the guys are supposed to get \"de-loused\" and for about three minutes, fully dressed, do some schtick. In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?), and for most, their butts, part or full backside, are shown. Was this an early variation of beefcake courtesy of Howard Hughes?']\n",
    "\n",
    "y= ['honey that movie was marvelous ']\n",
    "y = [' today ']\n",
    "y = [\"William Boyd and Louis Wolheim are the \"\"Two Arabian Knights\"\" referred to in the title, humorously. The pair start out as U.S. POWs trying to escape from the Germans during World War I. Eventually, they find themselves on board a ship bound for Arabia. While tripping out to the Middle East, they rescue an Arab woman, Mary Astor, who turns out to be a Princess; and, of course, becomes a romantic interest for the Two Arabian Knights. No points for guessing who wins the veiled Ms. Astor!<br /><br />The film is very well photographed and directed; Lewis Milestone has wonderful sets, and stages scenes beautifully. Of the performances, Mr. Wolheim stands out - he creates a character so understandable you can almost hear him speak, trough the film is silent. The story isn't as strong as it could be - there are some events and sequences which had me wondering how and why the characters' locale changed. The last looks, exchanged between one of the stars and an extra, is an example of something I didnt understand. Perhaps these were comic bits which had a particular appeal for the time.<br /><br />The film is damaged in several places; but there is enough preserved, in even these scenes, to allow your mind to fill in the visual blanks. Boris Karloff appears as the Purser; watch for his big scene on ship, when Wolheim goes into a room with him for some money (what actually happens is a mystery). Early in the film, there is a long scene with a lot of naked men shown from the waist up (or, thereabouts); they are POWs being herded to the showers. Director Milestone uses parades of soldiers moving to great effect; this shower scene is different in that several of the men don't look as Caucasian as you might expect - maybe not as many Caucasian men would agree to appear nude? <br /><br />******* Two Arabian Knights (9/23/27) Lewis Milestone ~ William Boyd, Louis Wolheim, Mary Astor\"]\n",
    "y = [\"worst\"]\n",
    "y = [\"food was okay only. but i am sad.it's okay \"]\n",
    "y = [\"i am not sad with food of our college\"]\n",
    "y = ['sad is that he died. it was boring']\n",
    "y = ['Right now Im mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too']\n",
    "y = ['I dont know if its the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind! We picked up a bottle once on a trip we were on...']\n",
    "y = ['Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".']\n",
    "y = ['I love eating them and they are good for watching TV and looking at movies! It is not too sweet. I like to transfer them to a zip lock baggie so they stay fresh so I can take my time eating them.']\n",
    "y = ['The candy is just red , No flavor . Just plan and chewy . I would never buy them again']\n",
    "y = ['The flavors are good. However, I do not see any differce between this and Oaker Oats brand - they are both mushy']\n",
    "\n",
    "y = ['Buyer Beware Please! This sweetener is not for everybody. Maltitol is an alcohol sugar and can be undigestible in the body. You will know a short time after consuming it if you are one of the unsusp...']\n",
    "y = ['Food was very tasty. Though, they maintain less hygiene, but you can go for once and all for having tasty food while returning from tiring journey.']\n",
    "y = ['mess secretary please improve']\n",
    "k = sequence_of_single_sentence(y, tokenizer)\n",
    "print(k.shape)\n",
    "\n",
    "\n",
    "\n",
    "model.predict(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03460965]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "y = 'hello mess improve'\n",
    "x.append(y)\n",
    "k = sequence_of_single_sentence(x, tokenizer)\n",
    "model.predict(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model =getModelFunny()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_on_new_train.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelFunny():\n",
    "    model = load_model('model_rnn_pretrained_trained.h5')\n",
    "    return model\n",
    "def getModelSentiment():\n",
    "    return load_model('rnn_sentiment_86.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sequence_of_single_sentence(sentence, tokenizer, MAX_SEQUENCE_LENGTH = 500):\n",
    "    sen = tokenizer.texts_to_sequences(sentence)\n",
    "    return sequence.pad_sequences(sen, maxlen= MAX_SEQUENCE_LENGTH)\n",
    "Feedback = 'not good'\n",
    "x = [str(Feedback)]\n",
    "\n",
    "y = np.asarray(x)\n",
    "\n",
    "k = sequence_of_single_sentence(y, tokenizer)\n",
    "\n",
    "model_Sent = getModelSentiment()\n",
    "print(\"model loaded\")\n",
    "out = model_Sent.predictSentiment(k)\n",
    "print(\"out \",out)\n",
    "sentiment_star = out[0][1]\n",
    "print(sentiment_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
