{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. making a RNN pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "    print(imdb_data_path)\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    #test_texts = []\n",
    "    #test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return (train_texts, np.array(train_labels) )\n",
    "\n",
    "\n",
    "\n",
    "def sequence_vectorize(train_texts):\n",
    "    # Vectorization parameters\n",
    "    # Limit on the number of features. We use the top 20K features.\n",
    "    TOP_K = 20000\n",
    "    # Limit on the length of text sequences. Sequences longer than this\n",
    "    # will be truncated.\n",
    "    MAX_SEQUENCE_LENGTH = 500\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    #x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    #x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, tokenizer\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rohit/Documents/Study/Projects/HACKATHON INNOVATE FOR IIT/aclImdb_v1/aclImdb\n",
      "train_text length  50000\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "# get the sequences\n",
    "#define useful global variables\n",
    "import pickle\n",
    "TOP_K = 20000\n",
    "MAX_SEQUENCE_LENGTH  = 500\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "data_path = '/home/rohit/Documents/Study/Projects/HACKATHON INNOVATE FOR IIT/aclImdb_v1'\n",
    "(train_texts, train_labels) = load_imdb_sentiment_analysis_dataset(data_path)\n",
    "\n",
    "print('train_text length ', len(train_texts))\n",
    "print(train_labels.shape)\n",
    "#sequences\n",
    "\n",
    "    \n",
    "x_train, tokenizer = sequence_vectorize(train_texts)\n",
    "\n",
    "with open('tokenizer_on_IMDB.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "num_words = min(TOP_K, len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#get the glove embeedings\n",
    "glove_dir = '/home/rohit/Documents/Study/Projects/HACKATHON INNOVATE FOR IIT/'\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= TOP_K:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = embedding_matrix\n",
    "max_words = MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM):\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), EMBEDDING_DIM, weights=[weight_matrix], input_length=max_words, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohit/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Adam Optimiser\n",
    "#some hyperparameters\n",
    "model_rnn = create_model_rnn(weight_matrix = embedding_matrix, max_words= MAX_SEQUENCE_LENGTH, EMBEDDING_DIM = 100)\n",
    "\n",
    "learning_rate=1e-3,\n",
    "epochs=1000,\n",
    "batch_size= 128,\n",
    "blocks=2,\n",
    "filters=64,\n",
    "dropout_rate= 0.5,\n",
    "embedding_dim=EMBEDDING_DIM,\n",
    "kernel_size=3,\n",
    "pool_size=3\n",
    "\n",
    "num_classes = 2\n",
    "num_features = num_words\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "model_rnn.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 168s 3ms/step - loss: 0.6347 - acc: 0.6321\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 167s 3ms/step - loss: 0.5062 - acc: 0.7529\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 166s 3ms/step - loss: 0.4018 - acc: 0.8183\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 166s 3ms/step - loss: 0.3652 - acc: 0.8387\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 166s 3ms/step - loss: 0.3455 - acc: 0.8495\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 168s 3ms/step - loss: 0.3288 - acc: 0.8574\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 168s 3ms/step - loss: 0.3182 - acc: 0.8624\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 172s 3ms/step - loss: 0.3083 - acc: 0.8676\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 0.3030 - acc: 0.8723\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 174s 3ms/step - loss: 0.2944 - acc: 0.8741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1358215ed0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train,train_labels, epochs= 10, verbose = 1, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.save(\"model_rnn_pretrained.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "\n",
    "df = pd.read_csv('./Reviews Data/train.csv', index_col= 0)\n",
    "#df_test = pd.read_csv('./Reviews Data/data_test1.csv', index_col = 0)\n",
    "#df.head(10)\n",
    "\n",
    "df_train_reviews = list(df.text[:9000])\n",
    "df_train_labels = np.asarray(df.funny[:9000]).reshape(len(df_train_reviews), 1)\n",
    "\n",
    "df_test_reviews = list(df.text[9000:])\n",
    "df_test_labels = np.asarray(df.funny[9000:]).reshape(len(df_test_reviews),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_pretrained = load_model('model_rnn_pretrained.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    # Create vocabulary with training texts.\n",
    "    MAX_SEQUENCE_LENGTH = 500\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed ready to train data\n",
    "\n",
    "x_train,x_test = sequence_vectorize(df_train_reviews, df_test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "[0.9542548818588257, 0.454]\n",
      "9000/9000 [==============================] - 21s 2ms/step\n",
      "[0.9570432047843933, 0.45844444444444443]\n"
     ]
    }
   ],
   "source": [
    "print(model_pretrained.evaluate(x_test, df_test_labels, batch_size = 64))\n",
    "print(model_pretrained.evaluate(x_train, df_train_labels, batch_size = 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.4244 - acc: 0.8148\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.4229 - acc: 0.8164\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.4132 - acc: 0.8189\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.4129 - acc: 0.8213\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.4069 - acc: 0.8249\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.4043 - acc: 0.8224\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3987 - acc: 0.8261\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3952 - acc: 0.8282\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3944 - acc: 0.8268\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3820 - acc: 0.8334\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3803 - acc: 0.8364\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3689 - acc: 0.8409\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3645 - acc: 0.8430\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3553 - acc: 0.8451\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3822 - acc: 0.8418\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3565 - acc: 0.8439\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3449 - acc: 0.8497\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3455 - acc: 0.8521\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3331 - acc: 0.8567\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3304 - acc: 0.8541\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3198 - acc: 0.8628\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3262 - acc: 0.8596\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3181 - acc: 0.8599\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3096 - acc: 0.8644\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3057 - acc: 0.8672\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.3025 - acc: 0.8698\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2928 - acc: 0.8752\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2822 - acc: 0.8784\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2883 - acc: 0.8760\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2791 - acc: 0.8832\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2695 - acc: 0.8831\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2702 - acc: 0.8861\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2609 - acc: 0.8888\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2581 - acc: 0.8898\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2595 - acc: 0.8910\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2463 - acc: 0.8958\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2483 - acc: 0.8960\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2444 - acc: 0.8981\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2313 - acc: 0.9001\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2430 - acc: 0.9004\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2284 - acc: 0.9066\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2256 - acc: 0.9062\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2280 - acc: 0.9077\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2278 - acc: 0.9041\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2201 - acc: 0.9076\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2222 - acc: 0.9089\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2161 - acc: 0.9074\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2127 - acc: 0.9090\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2114 - acc: 0.9133\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1992 - acc: 0.9177\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2060 - acc: 0.9147\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1933 - acc: 0.9198\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1983 - acc: 0.9182\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1969 - acc: 0.9189\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1975 - acc: 0.9176\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1841 - acc: 0.9250\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1917 - acc: 0.9224\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1808 - acc: 0.9249\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1798 - acc: 0.9276\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1837 - acc: 0.9272\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1784 - acc: 0.9258\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1855 - acc: 0.9203\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1710 - acc: 0.9336\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1718 - acc: 0.9278\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1700 - acc: 0.9297\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1640 - acc: 0.9318\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1708 - acc: 0.9308\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1697 - acc: 0.9322\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1604 - acc: 0.9352\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1522 - acc: 0.9362\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1604 - acc: 0.9352\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1534 - acc: 0.9352\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1586 - acc: 0.9381\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1542 - acc: 0.9381\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1546 - acc: 0.9372\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1515 - acc: 0.9371\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1474 - acc: 0.9418\n",
      "Epoch 78/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1552 - acc: 0.9386\n",
      "Epoch 79/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1435 - acc: 0.9407\n",
      "Epoch 80/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1442 - acc: 0.9428\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1384 - acc: 0.9441\n",
      "Epoch 82/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1439 - acc: 0.9408\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1338 - acc: 0.9486\n",
      "Epoch 84/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1445 - acc: 0.9417\n",
      "Epoch 85/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1392 - acc: 0.9447\n",
      "Epoch 86/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1426 - acc: 0.9426\n",
      "Epoch 87/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1401 - acc: 0.9458\n",
      "Epoch 88/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1257 - acc: 0.9491\n",
      "Epoch 89/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1342 - acc: 0.9436\n",
      "Epoch 90/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1304 - acc: 0.9469\n",
      "Epoch 91/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1392 - acc: 0.9458\n",
      "Epoch 92/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1233 - acc: 0.9513\n",
      "Epoch 93/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1287 - acc: 0.9484\n",
      "Epoch 94/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1243 - acc: 0.9509\n",
      "Epoch 95/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1307 - acc: 0.9472\n",
      "Epoch 96/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1277 - acc: 0.9493\n",
      "Epoch 97/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1206 - acc: 0.9514\n",
      "Epoch 98/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1224 - acc: 0.9501\n",
      "Epoch 99/100\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1221 - acc: 0.9520\n",
      "Epoch 100/100\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1230 - acc: 0.9520\n"
     ]
    }
   ],
   "source": [
    "model_pretrained.fit(x_train, df_train_labels, batch_size = 128,epochs = 100,shuffle=True)\n",
    "model_pretrained.save('model_rnn_pretrained_trained2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained.save('model_rnn_pretrained_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2028902244567872, 0.7649999995231629]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pretrained.evaluate(x_test, df_test_labels, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 21s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39871785895029704, 0.8224444443384806]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pretrained.evaluate(x_train, df_train_labels, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
